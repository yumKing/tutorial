git操作：
1、github上创建一个仓库
2、git remote add origin 仓库地址
3、git pull origin master(第一次报错)
	1、fatal: Couldn't find remote ref master 不用管，继续提交
	2、git remote rm origin
	   git remote add origin XXXX
4、git push -u origin master

=======================
git安装配置:

用户配置:
git config --global user.name xxx
git config --global user.email xxx

文本编辑器配置:(默认不用设置)
git config --global core.editor emacs

差异分析工具配置(默认不用设置)
git config --global merge.tool vimdiff

检查已有的配置信息
git config --list

1、提交说明的撰写:
	50字简明扼要的描述更新内容，空开一行再展开详细注解(包括本次修订的原由，以及不同实现之间的比较)
	如:
	===============
	本次更新的简要描述（50 个字符以内）

	如果必要，此处展开详尽阐述。段落宽度限定在 72 个字符以内。
	某些情况下，第一行的简要描述将用作邮件标题，其余部分作为邮件正文。
	其间的空行是必要的，以区分两者（当然没有正文另当别论）。
	如果并在一起，rebase 这样的工具就可能会迷惑。

	另起空行后，再进一步补充其他说明。

 		- 可以使用这样的条目列举式。

 		- 一般以单个空格紧跟短划线或者星号作为每项条目的起始符。每个	条目间用一空行隔开。
   			不过这里按自己项目的约定，可以略作变化。
	===============
2、私有项目管理:源代码不公开,具有推送数据到仓库的权限
git:离线提交，快速分支与合并等等,合并操作发生在客户端

=====================================================
scrapy 安装
使用pip安装scrapy不成功,则:（安装whl之前先安装wheel）
下载scrapy whl文件
下载scrapy依赖库twiste whl文件

1、安装pip install twiste.whl
2、安装scrapy相关依赖包,直接使用pip安装即可
3、安装scrapy
4、测试安装成功：scrapy -h

=====================================================

1、创建项目：scrapy startproject tutorial
	scrapy.cfg:项目配置文件
	tutorial:项目python模块
	tutorial/item.py:项目的item文件
	tutorial/pipelines.py:项目的pipelines文件
	tutorial/settings.py:项目的设置文件
	tutorial/spiders:防止spider代码的模块

2、item定义：保存爬取到的数据的容器
	 模式:field = scrapy.Field()

3、spiders:
	定义一个类集成scrapy.Spider
		name ：必须定义，且唯一
		start_urls:(list类型) 初始要爬取的url列表
		parse():参数为url的响应结果对象response，返回生成的item和进一步处理的url的request对象

4、爬取
进入项目根目录，运行 scrapy crawl name  # 其中name为要爬取的spider的唯一名字


5、获取数据


others:
1、运行scrapy crawl name 报错
提示 ImportError: No module named 'win32api'
需要安装 pip install pypiwin32

2、xpath
/html/body/div/div[2]/div[1]/div[1]
body > div > div:nth-child(2) > div.col-md-8 > div:nth-child(1)